{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "weird-general",
   "metadata": {},
   "source": [
    "# EAs Goodreads Analysis\n",
    "### Questions:\n",
    "- ~~what are most popular books~~\n",
    "- ~~what are most to-read but not read books~~\n",
    "- ~~highest/lowest rated~~\n",
    "- average books read per year\n",
    "- books that many people read before EA was a thing\n",
    "- ~~books that are relatively fringe but read by EAs~~\n",
    "- express the numbers as percentages, too\n",
    "\n",
    "### ToDo:\n",
    "- it looks like for a few profiles, e.g. \"117194676\" and \"52226471\" my program didn't return the books even though their profiles are public\n",
    "    - I might have messed up something when I split up the scraping into multiple sessions\n",
    "\n",
    "### Scraping ethics\n",
    "- as far as I can see, all pages I scrape are not disallowed: https://www.goodreads.com/robots.txt\n",
    "    - (weirdly there seem to be ~200 books that are individually not allowed to be scraped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "exclusive-deviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "from dateutil import parser\n",
    "import lxml\n",
    "import pandas as pd\n",
    "import random\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "nearby-vegetable",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.goodreads.com/group/151274-effective-altruists/members'\n",
    "#response = requests.get(url)\n",
    "#response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "present-injury",
   "metadata": {},
   "outputs": [],
   "source": [
    "#soup = BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-relative",
   "metadata": {},
   "source": [
    "### Get all user sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "foreign-flavor",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = []\n",
    "for i in range(1,13):\n",
    "    time.sleep(random.uniform(16,34))\n",
    "    response = requests.get(url + \"?page=\" + str(i))\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    all_tags = soup.findAll(\"a\", attrs={\"class\": \"userName\"})\n",
    "    all_refs = []\n",
    "    for tag in all_tags:\n",
    "        all_refs.append(tag.get(\"href\"))\n",
    "    users = users + all_refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "further-pakistan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "333"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "confirmed-silence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/user/show/85957352-kiim</td>\n",
       "      <td>85957352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/user/show/92096951-max</td>\n",
       "      <td>92096951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/user/show/38695642-fin-moorhouse</td>\n",
       "      <td>38695642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/user/show/5583842-radovan-kavick</td>\n",
       "      <td>5583842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/user/show/124437386-fiona</td>\n",
       "      <td>124437386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 URL         ID\n",
       "0           /user/show/85957352-kiim   85957352\n",
       "1            /user/show/92096951-max   92096951\n",
       "2  /user/show/38695642-fin-moorhouse   38695642\n",
       "3  /user/show/5583842-radovan-kavick    5583842\n",
       "4         /user/show/124437386-fiona  124437386"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_users = pd.DataFrame(users, columns=[\"URL\"])\n",
    "#df_users[\"ID\"] = df_users[\"URL\"].str.replace(r\"\\D\", '', regex=True)\n",
    "#df_users.to_csv(\"GoodreadsEAs.csv\")\n",
    "df_users = pd.read_csv(\"GoodreadsEAs.csv\")\n",
    "df_users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-rates",
   "metadata": {},
   "source": [
    "### Get books\n",
    "#### Plan\n",
    "- go through each EA one by one\n",
    "- go through their books\n",
    "- save\n",
    "    - read or not\n",
    "    - if rated: rating\n",
    "    - if read: (first) date it was read\n",
    "- data structure:\n",
    "    - one df for each user\n",
    "    - one row for each book\n",
    "    - columns for read/to-read, rating, date\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-turning",
   "metadata": {},
   "source": [
    "#### How to navigate along the page\n",
    "- Format of list of books: https://www.goodreads.com/review/list/{userID}\n",
    "- Master list doesn't have reading status info, seems like I have to go seperately through \"read\", \"currently reading\" and \"to read\"\n",
    "    - ?shelf=read\n",
    "    - ?shelf=currently-reading\n",
    "    - ?shelf=to-read\n",
    "- &page={i}\n",
    "    - starting with 1\n",
    "    - ends with \"No books matching\"\n",
    "- &per_page=100\n",
    "    - Goodreads gives me 30 books per page no matter what, weird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "exterior-gardening",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_books(soup, userID, shelf):\n",
    "    \"\"\"Gets soup of whole page, returns df with books of that page.\"\"\"\n",
    "    all_books = soup.findAll(\"tr\", attrs={\"class\": \"bookalike review\"})\n",
    "    \n",
    "    titles = []\n",
    "    alt_titles = []\n",
    "    authors = []\n",
    "    avg_ratings = []\n",
    "    num_reviews_list = []\n",
    "    ratings = []\n",
    "    dates_added = []\n",
    "    dates_read = []\n",
    "    \n",
    "    \n",
    "    for book in all_books:\n",
    "        title = book.findAll(\"td\", attrs={\"class\": \"field title\"})[0].findAll(\"a\")[0].string\n",
    "        titles.append(title)\n",
    "\n",
    "        alt_title = book.findAll(\"td\", attrs={\"class\": \"field title\"})[0].findAll(\"a\")[0][\"title\"]\n",
    "        alt_titles.append(alt_title)\n",
    "\n",
    "        author = book.findAll(\"td\", attrs={\"class\": \"field author\"})[0].findAll(\"a\")[0].string\n",
    "        authors.append(author)\n",
    "\n",
    "        avg_rating = book.findAll(\"td\", attrs={\"class\": \"avg_rating\"})[0].findAll(\"div\")[0].string\n",
    "        avg_rating = float(avg_rating)\n",
    "        avg_ratings.append(avg_rating)\n",
    "\n",
    "        num_reviews = book.findAll(\"td\", attrs={\"class\": \"field num_ratings\"})[0].findAll(\"div\")[0].string\n",
    "        num_reviews = int(num_reviews.replace(\",\", \"\"))\n",
    "        num_reviews_list.append(num_reviews)\n",
    "\n",
    "        # rating - catching unrated books\n",
    "        try:\n",
    "            rating = book\\\n",
    "                    .findAll(\"td\", attrs={\"class\": \"field rating\"})[0]\\\n",
    "                    .findAll(\"span\", attrs={\"class\": \"staticStars notranslate\"})[0][\"title\"]\n",
    "        except KeyError:\n",
    "            rating = None\n",
    "        ratings.append(rating)\n",
    "\n",
    "        # date added - catching undated books\n",
    "        date_added = book.findAll(\"td\", attrs={\"class\": \"field date_added\"})[0].findAll(\"span\")[0].string\n",
    "        try:\n",
    "            date_added = parser.parse(date_added)\n",
    "        except:\n",
    "            date_added = \"not set\"\n",
    "        dates_added.append(date_added)\n",
    "\n",
    "        # date read - catching unread books\n",
    "        date_read = book.findAll(\"td\", attrs={\"class\": \"field date_read\"})[0].findAll(\"span\")[0].string\n",
    "        try:\n",
    "            date_read = parser.parse(date_read)\n",
    "        except:\n",
    "            date_read = \"not set\"\n",
    "        dates_read.append(date_read)\n",
    "\n",
    "        \n",
    "    # these all should be the same length\n",
    "    assert len(titles) == len(alt_titles) == len(authors) == len(avg_ratings) == len(num_reviews_list) == len(ratings) == len(dates_added) == len(dates_read)\n",
    "    \n",
    "    d = {\"userID\": [userID]*len(titles), \"shelf\": [shelf]*len(titles), \"title\": titles, \"alt_title\": alt_titles, \"author\": authors, \"avg_rating\": avg_ratings, \"num_reviews\": num_reviews_list,\n",
    "        \"rating\": ratings, \"date_added\": dates_added, \"date_read\": dates_read}\n",
    "    \n",
    "    return pd.DataFrame(d)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "prime-charm",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"userID\": [\"Test ID\"], \"shelf\": \"to-test\", \"title\": [\"Test Title\"], \"alt_title\": [\"Test alt. Title\"], \"author\": [\"Test Author\"], \"avg_rating\": [\"3.33\"], \"num_reviews\": [\"69\"],\n",
    "        \"rating\": [\"liked it\"], \"date_added\": [\"March 3rd, 1933\"], \"date_read\": [\"May 4th, 1999\"]}\n",
    "df_books = pd.DataFrame(d)\n",
    "#df_books\n",
    "#df_books = pd.read_csv(\"GoodreadsEAs_books.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-absence",
   "metadata": {},
   "source": [
    "### Go through all users\n",
    "This got a little messy because Goodreads now and then bounced me so I split up the userIDs in multiple lists.\n",
    "to_scrape should just be list(set(df_users[\"ID\"])) if one wants to start this again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "continued-circuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to_scrape = list(set(df_users[\"ID\"])^set(df_books[\"userID\"].unique()))\n",
    "#test = pd.Series(to_scrape)\n",
    "#test.to_csv(\"to_scrape.csv\")\n",
    "#to_scrape = read_csv(\"to_scrape.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "auburn-darwin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to_scrape1 = to_scrape[0:60]\n",
    "#to_scrape2 = to_scrape[60:120]\n",
    "#to_scrape3 = to_scrape[120:180]\n",
    "#to_scrape4 = to_scrape[180:240]\n",
    "#to_scrape5 = to_scrape[240:300]\n",
    "#to_scrape6 = to_scrape[300:]\n",
    "#missing = [\"68316850\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "chinese-pearl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 68316850\n",
      "1-2-3-4-5-6-7-8-9-10-11-12-13-14-15-16-17-18-19-20-21-22-23-24-25-26-27-28-29-30-31-32-33-34-35-36-37-38-39-40-1-1-2-3-4-5-6-7-8-9-10-11-12-13-14-15-16-17-18-19-20-21-22-23-24-25-26-27-28-29-30-31-32-33-34-35-36-37-38-39-40-41-42-43-44-"
     ]
    }
   ],
   "source": [
    "base = \"https://www.goodreads.com/review/list/\"\n",
    "for userID in missing:\n",
    "    print(\"\\n\", userID)\n",
    "    userURL = base + userID\n",
    "    for shelf in [\"read\", \"currently-reading\", \"to-read\"]:\n",
    "        shelfURL = userURL + \"?shelf=\" + shelf\n",
    "        for page in range(1, 200): # don't want to do a while-loop, the super bookworms require ~150 loops\n",
    "            print(page, end=\"-\")\n",
    "            pageURL = shelfURL + \"&page=\" + str(page)\n",
    "            # pageURL += \"&per_page=100\" # for some reason Goodreads returns 30 per page no matter what I call\n",
    "            \n",
    "            time.sleep(random.uniform(22,32))\n",
    "            \n",
    "            response = requests.get(pageURL)\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            extracted = extract_books(soup, userID, shelf)\n",
    "            df_books = df_books.append(extracted)\n",
    "            \n",
    "            if extracted.shape[0] < 5:\n",
    "                break\n",
    "    df_books.to_csv(\"books1.csv\")\n",
    "    time.sleep(random.uniform(12,24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-strand",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
